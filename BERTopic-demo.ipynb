{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5ecfe63",
   "metadata": {},
   "source": [
    "# BERTopic Short Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38cfece",
   "metadata": {},
   "source": [
    "If you are working on Colab, \n",
    "- The following cell installs all the packages you will need. \n",
    "- You may want to make use of the (free) GPU resources: click on the down arrow in the upper-right of the page next to the RAM and Disk usage graphic.  Then \"Change runtime type\" and select \"T4 GPU\".  This will dramatically speed up your runtime for this code.\n",
    "- Please be sure to save your file on your own account. (If you clicked on the link on our GitHub repo, your changes are not saved automatically).\n",
    "\n",
    "If you are working locally on your computer, please see the [README.md](https://github.com/nuitrcs/AI_Week_Topic_Model/blob/main/README.md) file on our GitHub repo for a command to create a conda environment that has the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b25c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    print(\"You are working in Google Colab.  We will install necessary packages...\")\n",
    "    !pip install scikit-learn sentence-transformers umap-learn hdbscan bertopic pandas matplotlib datashader bokeh holoviews scikit-image colorcet keybert\n",
    "except:\n",
    "    print(\"You are not working in Google Colab.\")\n",
    "    print(\"Please be sure that the necessary packages are installed and available, ideally within a conda env (e.g., see here: https://github.com/nuitrcs/AI_Week_Topic_Model/blob/main/README.md).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed3649d",
   "metadata": {},
   "source": [
    "### Read and Preprocess Data\n",
    "\n",
    "For this demo, we will use the [`20newsgroups` dataset from scikit-learn](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset).  We will fetch the data and then reformat it so that it is easier for BERTopic to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee9b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "# this is a function from sklearn that fetches the 20 newsgroups text dataset\n",
    "# it is a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups\n",
    "# this returns a bunch object, which is very similar to a dictionary\n",
    "bunch = fetch_20newsgroups(\n",
    "    categories=[\"comp.graphics\", \"rec.autos\", \"rec.motorcycles\", \n",
    "                \"rec.sport.baseball\", \"rec.sport.hockey\", \n",
    "                \"sci.electronics\", \"sci.med\", \"sci.space\"], # only extract select topics\n",
    "    remove=(\"headers\",\"footers\",\"quotes\")) # don't extract unnecessary metadata\n",
    "\n",
    "# get the text data and labels\n",
    "docs = bunch[\"data\"]\n",
    "doc_labels = bunch[\"target\"]\n",
    "\n",
    "print(\"Documents: \")\n",
    "print(docs[:5])\n",
    "\n",
    "# create a data frame with the text and labels\n",
    "df = pd.DataFrame({\n",
    "    \"text\": docs,\n",
    "    \"labels\": doc_labels\n",
    "})\n",
    "\n",
    "# create a label with text info\n",
    "df[\"labels_text\"] = df[\"labels\"].astype(\"category\").cat.rename_categories({i:j for i,j in enumerate(bunch[\"target_names\"])})\n",
    "\n",
    "print()\n",
    "print(\"Data Frame: \")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2651fff8",
   "metadata": {},
   "source": [
    "Before applying topic modeling to the text, we should do a basic preprocessing, mainly stripping of newlines and removing empty texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5300717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip blank characters\n",
    "df[\"text_processed\"] = df[\"text\"].str.strip()\n",
    "\n",
    "# remove empty text from data frame\n",
    "empty_text_bool =  df[\"text_processed\"].str.len() == 0\n",
    "\n",
    "print(f\"Number of empty texts: {empty_text_bool.sum()}\")\n",
    "\n",
    "# remove empty text from df\n",
    "df = df[~empty_text_bool]\n",
    "\n",
    "print(\"Final Data Frame:\")\n",
    "print(f\"Dimension: {df.shape[0]}, {df.shape[1]}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c0271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the texts into docs variable\n",
    "docs = df[\"text_processed\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b2331",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d4a35",
   "metadata": {},
   "source": [
    "### Simplest case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984ed7ca",
   "metadata": {},
   "source": [
    "BERTopic can be run out of the box without any tuning. However, this doesn't guarantee the best number of topics and representation for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic() # initialize the model\n",
    "topic_model.fit(docs) # fit the model to the data\n",
    "\n",
    "topic_model.get_topic_info() # get the topic information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28592a02",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "This step uses a language model to convert the text into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# initialize model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\") # all-MiniLM-L6-v2 is name of pretrained model\n",
    "embeddings = embedding_model.encode(docs) # encode the texts into embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5fe210",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimension of embeddings: \")\n",
    "print(embeddings.shape)\n",
    "print()\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab168fc",
   "metadata": {},
   "source": [
    "### Dimension Reduction\n",
    "\n",
    "This step uses the [UMAP](https://umap-learn.readthedocs.io/en/latest/index.html) library to reduce the dimensions to 2 (to make it easier to cluster the data in the next step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772fcca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import umap.plot\n",
    "\n",
    "# set random seed for reproducibility\n",
    "seed = 54382\n",
    "# initialize UMAP model\n",
    "umap_model = UMAP(n_components=2, n_neighbors = 15, metric=\"cosine\", random_state=seed)\n",
    "# fit the UMAP model to find the best 2D representation of the embeddings\n",
    "umap_model.fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a9393",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimension of UMAP output: \")\n",
    "print(umap_model.embedding_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d273eb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the UMAP representation\n",
    "umap.plot.points(umap_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cb0374",
   "metadata": {},
   "source": [
    "### Unsupervised Clustering\n",
    "\n",
    "Here we use the [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/#) library to identify clusters in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61384414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# initialize HDBSCAN model\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, min_samples=1, cluster_selection_epsilon=0.165)\n",
    "\n",
    "# identify clusters on the 2-d representation of embeddings generated by UMAP\n",
    "hdbscan_model.fit(umap_model.embedding_)\n",
    "umap.plot.points(umap_model, labels=hdbscan_model.labels_, theme=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c068f314",
   "metadata": {},
   "source": [
    "### Labeling\n",
    "\n",
    "Here we label each cluster using another language model with [KeyBERT](https://maartengr.github.io/KeyBERT/api/keybert.html).  Note that this is similar to, though not identical, to what BERTopic uses (e.g., see the BERTopic documentation [here](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#keybertinspired))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3122454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "import numpy as np\n",
    "\n",
    "# initialize the model; can use the same LM as we used for embeddings\n",
    "rep_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "\n",
    "# loop through the clusters and get the labels (as BERTopic would do)\n",
    "for label in np.unique(hdbscan_model.labels_):\n",
    "    # Get docs in this cluster\n",
    "    cluster_docs = [doc for doc, c in zip(docs, hdbscan_model.labels_) if c == label]\n",
    "    # Combine documents into a single string\n",
    "    combined_text = ' '.join(cluster_docs)\n",
    "    # Extract keywords\n",
    "    keywords = rep_model.extract_keywords(combined_text, top_n=5)\n",
    "    # print the results \n",
    "    # Note: KeyBERT returns a tuple with the (word, numer), where the number is:\n",
    "    #   the relevance score, i.e., the cosine similarity between the embedding of the keyword and the original doc\n",
    "    print(label, [kw[0] for kw in keywords])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0bbb79",
   "metadata": {},
   "source": [
    "## Combine All Steps with BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beec9623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "# set random seed for reproducibility\n",
    "seed = 54382\n",
    "\n",
    "# embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\") # all-MiniLM-L6-v2 is name of pretrained model\n",
    "\n",
    "# umap model\n",
    "umap_model = UMAP(n_components=2, n_neighbors = 15, metric=\"cosine\", random_state=seed)\n",
    "\n",
    "# initialize HDBSCAN model\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, min_samples=1, cluster_selection_epsilon=0.165)\n",
    "\n",
    "# representation model\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# define the BERTopic model using the models above\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    representation_model=representation_model,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# fit the model to the data\n",
    "topic_model.fit(docs) \n",
    "\n",
    "# get the topic information\n",
    "topic_model.get_topic_info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3489b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
