{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5ecfe63",
   "metadata": {},
   "source": [
    "# BERTopic Short Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38cfece",
   "metadata": {},
   "source": [
    "If you are working on Colab, \n",
    "- The following cell installs all the packages you will need. \n",
    "- You may want to make use of the (free) GPU resources: click on the down arrow in the upper-right of the page next to the RAM and Disk usage graphic.  Then \"Change runtime type\" and select \"T4 GPU\".  This will dramatically speed up your runtime for this code.\n",
    "- Please be sure to save your file on your own account. (If you clicked on the link on our GitHub repo, your changes are not saved automatically).\n",
    "\n",
    "If you are working locally on your computer, please see the [README.md](https://github.com/nuitrcs/AI_Week_Topic_Model/blob/main/README.md) file on our GitHub repo for a command to create a conda environment that has the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b25c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    print(\"You are working in Google Colab.  We will install necessary packages...\")\n",
    "    !pip install scikit-learn sentence-transformers umap-learn hdbscan bertopic pandas matplotlib datashader bokeh holoviews scikit-image colorcet keybert\n",
    "except:\n",
    "    print(\"You are not working in Google Colab.\")\n",
    "    print(\"Please be sure that the necessary packages are installed and available, ideally within a conda env (e.g., see here: https://github.com/nuitrcs/AI_Week_Topic_Model/blob/main/README.md).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed3649d",
   "metadata": {},
   "source": [
    "### Read in the Preprocessed Data\n",
    "\n",
    "For this demo, we will use the [`20newsgroups` dataset from scikit-learn](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset).  We will fetch the data and then clean reformat it so that it is easier for BERTopic to work with.  I wrote code to do this in the `exercises/supplementary_code.ipynb` notebook; that code also saves the output to a .csv file.  We can simply read in the resulting cleaned data below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f42ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# The line below will read the file directly from GitHub and will work on Colab.  \n",
    "df = pd.read_csv('https://raw.githubusercontent.com/nuitrcs/AI_Week_Topic_Modeling/refs/heads/main/exercises/data/sklearn_20newsgroups_cleaned.csv')\n",
    "# If you're working on your local computer and prefer to simply read the file from your disk, you can instead use the line below.\n",
    "# df = pd.read_csv('exercises/data/sklearn_20newsgroups_cleaned.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c0271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the texts into docs variable as a list for use in BERTopic\n",
    "docs = df[\"cleaned_text\"].values.tolist()\n",
    "print(docs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d4a35",
   "metadata": {},
   "source": [
    "### Simplest case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984ed7ca",
   "metadata": {},
   "source": [
    "BERTopic can be run out of the box without any tuning. However, this doesn't guarantee the best number of topics and representation for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic() # initialize the model\n",
    "topic_model.fit(docs) # fit the model to the data\n",
    "\n",
    "topic_model.get_topic_info() # get the topic information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bb53bf",
   "metadata": {},
   "source": [
    "## Now let's break this down into the componet steps we discussed in the presentation:\n",
    "\n",
    "![graphical representation of topic modeling pipeline](exercises/images/topic_modeling_pipeline.png)\n",
    "\n",
    "1. Embeddings\n",
    "2. Dimension reduction\n",
    "3. Clustering\n",
    "4. Labeling\n",
    "\n",
    "Each of these steps have parameters we can tune.  This way we will have more fine-grained control so that we can improve the topics that are returned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28592a02",
   "metadata": {},
   "source": [
    "### 1. Embeddings\n",
    "\n",
    "![graphical representation of embedding step](exercises/images/embeddings.png)\n",
    "\n",
    "\n",
    "This step uses a language model to convert the text from the documents into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# initialize model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\") # all-MiniLM-L6-v2 is the name of a pretrained model\n",
    "embeddings = embedding_model.encode(docs) # encode the texts into embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5fe210",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimension of embeddings: \")\n",
    "print(embeddings.shape)\n",
    "print()\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab168fc",
   "metadata": {},
   "source": [
    "### 2. Dimension reduction\n",
    "\n",
    "![graphical representation of dimension reduction step](exercises/images/dimension_reduction.png)\n",
    "\n",
    "\n",
    "This step uses the [UMAP](https://umap-learn.readthedocs.io/en/latest/index.html) library to reduce the dimensions to 2 (to make it easier to cluster the data in the next step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772fcca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import umap.plot\n",
    "\n",
    "# set random seed for reproducibility\n",
    "seed = 42\n",
    "# initialize UMAP model\n",
    "umap_model = UMAP(n_components=2, n_neighbors = 15, metric=\"cosine\", random_state=seed)\n",
    "# fit the UMAP model to find the best 2D representation of the embeddings\n",
    "umap_model.fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a9393",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimension of UMAP output: \")\n",
    "print(umap_model.embedding_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d273eb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the UMAP representation\n",
    "umap.plot.points(umap_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cb0374",
   "metadata": {},
   "source": [
    "### 3. Clustering\n",
    "\n",
    "![graphical representation of clustering step](exercises/images/clustering.png)\n",
    "\n",
    "\n",
    "Here we use the [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/#) library, a suite of tools that uses unsupervised machine learning, to identify clusters in the data.  \n",
    "\n",
    "Note that you may see different clusters than other participants working on different computers because of the way each computer handles randomization.  But (hopefully!) your notebook will be internally consistent if you rerun it with the same random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61384414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# initialize HDBSCAN model\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=12, min_samples=5, cluster_selection_epsilon=0.2)\n",
    "\n",
    "# identify clusters on the 2-d representation of embeddings generated by UMAP\n",
    "hdbscan_model.fit(umap_model.embedding_)\n",
    "umap.plot.points(umap_model, labels=hdbscan_model.labels_, theme=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c068f314",
   "metadata": {},
   "source": [
    "### 4. Labeling\n",
    "\n",
    "![graphical representation of labeling step](exercises/images/labeling.png)\n",
    "\n",
    "Here we label each cluster using another language model with [KeyBERT](https://maartengr.github.io/KeyBERT/api/keybert.html).  Note that this is similar to, though not identical, to what BERTopic uses (e.g., see the BERTopic documentation [here](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#keybertinspired)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3122454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "import numpy as np\n",
    "\n",
    "# initialize the model; can use the same LM as we used for embeddings\n",
    "rep_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "\n",
    "hlabels = []\n",
    "counts = []\n",
    "words = []\n",
    "# loop through the clusters and get the labels (as BERTopic would do)\n",
    "for label in np.unique(hdbscan_model.labels_):\n",
    "    # Get docs in this cluster\n",
    "    cluster_docs = [doc for doc, c in zip(docs, hdbscan_model.labels_) if c == label]\n",
    "    # Combine documents into a single string\n",
    "    combined_text = ' '.join(cluster_docs)\n",
    "    # Extract keywords\n",
    "    keywords = rep_model.extract_keywords(combined_text, top_n=5)\n",
    "    # save the results \n",
    "    # Note: KeyBERT returns a tuple with the (word, numer), where the number is:\n",
    "    #   the relevance score, i.e., the cosine similarity between the embedding of the keyword and the original doc\n",
    "    hlabels.append(label)\n",
    "    counts.append(len(cluster_docs))\n",
    "    words.append([kw[0] for kw in keywords])\n",
    "\n",
    "# save this in a dataframe so it is prettier to look at (and easier to sort)\n",
    "output_df = pd.DataFrame({'hdbscan_label':hlabels, 'count':counts, 'keywords':words})\n",
    "# sort by counts (easier to compare to BERTopic output)\n",
    "output_df.sort_values(by=\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0bbb79",
   "metadata": {},
   "source": [
    "## Combine All Steps with BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beec9623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "# set random seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "# embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\") # all-MiniLM-L6-v2 is name of pretrained model\n",
    "\n",
    "# umap model\n",
    "umap_model = UMAP(n_components=2, n_neighbors = 15, metric=\"cosine\", random_state=seed)\n",
    "\n",
    "# initialize HDBSCAN model\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=12, min_samples=5, cluster_selection_epsilon=0.2)\n",
    "\n",
    "# representation model\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# define the BERTopic model using the models above\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    representation_model=representation_model,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# fit the model to the data\n",
    "topic_model.fit(docs) \n",
    "\n",
    "# get the topic information\n",
    "topic_model.get_topic_info() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab990f",
   "metadata": {},
   "source": [
    "### Additional visualization options from BERTopic\n",
    "\n",
    "BERTopic provides many different ways to visualization the results.  Here are some links to get your started.\n",
    "\n",
    "- [Visualize topics](https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_topics.html)\n",
    "- [Visualize documents](https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_documents.html)\n",
    "- [Additional info on best practices](https://maartengr.github.io/BERTopic/getting_started/best_practices/best_practices.html)\n",
    "\n",
    "We provide some examples below. Try running these for yourself and see what you get! You can use these methods to understand how your choices of model & parameters can affect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3489b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f63dee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that this uses the embeddings we calculated above\n",
    "topic_model.visualize_documents(docs, embeddings=embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
