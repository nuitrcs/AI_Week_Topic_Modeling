{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da7c1775",
   "metadata": {},
   "source": [
    "# Supplementary code for the BERTopic workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0918bdd9",
   "metadata": {},
   "source": [
    "## 20 news groups preprocessing\n",
    "\n",
    "We will use the [`20newsgroups` dataset from scikit-learn](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset).  The code below will fetch the data and then clean (strip newlines and remove empty texts) and reformat it so that it is easier for BERTopic to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd273ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08265a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_clean_20newsgroups(categories):\n",
    "    # this is a function from sklearn that fetches the 20 newsgroups text dataset\n",
    "    # it is a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups\n",
    "    # this returns a bunch object, which is very similar to a dictionary\n",
    "    bunch = fetch_20newsgroups(\n",
    "        categories=categories, # only extract select topics\n",
    "        remove=(\"headers\",\"footers\",\"quotes\")) # don't extract unnecessary metadata\n",
    "\n",
    "    # get the text data and labels\n",
    "    docs = bunch[\"data\"]\n",
    "    doc_labels = bunch[\"target\"]\n",
    "\n",
    "    # create a data frame with the text and labels\n",
    "    df = pd.DataFrame({\n",
    "        \"original_text\": docs,\n",
    "        \"labels\": doc_labels\n",
    "    })\n",
    "\n",
    "    # create a label with text info\n",
    "    label_number_to_text = {i: label for i, label in enumerate(bunch[\"target_names\"])}\n",
    "    df[\"labels_text\"] = df[\"labels\"].map(label_number_to_text)\n",
    "\n",
    "    # strip blank characters\n",
    "    df[\"cleaned_text\"] = df[\"original_text\"].str.strip()\n",
    "\n",
    "    # remove empty text from data frame\n",
    "    empty_text_bool =  df[\"cleaned_text\"].str.len() == 0\n",
    "\n",
    "    print(f\"Number of empty texts: {empty_text_bool.sum()}\")\n",
    "\n",
    "    # remove empty text from df\n",
    "    df = df[~empty_text_bool]\n",
    "\n",
    "    print(f\"Final dimension of dataset: {df.shape[0]}, {df.shape[1]}\")\n",
    "\n",
    "    df = df[[\"original_text\", \"cleaned_text\", \"labels\",\"labels_text\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fc48f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_and_clean_20newsgroups(\n",
    "    [\"comp.graphics\", \"rec.autos\", \"rec.motorcycles\", \n",
    "    \"rec.sport.baseball\", \"rec.sport.hockey\", \n",
    "    \"sci.electronics\", \"sci.med\", \"sci.space\"]\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7803602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data to a file\n",
    "df.to_csv('data/sklearn_20newsgroups_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6950bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
