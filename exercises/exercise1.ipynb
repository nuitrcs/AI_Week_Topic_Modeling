{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01aa8ac1",
   "metadata": {},
   "source": [
    "# BERTopic Exercise Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd14e36",
   "metadata": {},
   "source": [
    "If you are working on Colab, \n",
    "- The following cell installs all the packages you will need. \n",
    "- You may want to make use of the (free) GPU resources: click on the down arrow in the upper-right of the page next to the RAM and Disk usage graphic.  Then \"Change runtime type\" and select \"T4 GPU\".  This will dramatically speed up your runtime for this code.\n",
    "- Please be sure to save your file on your own account. (If you clicked on the link on our GitHub repo, your changes are not saved automatically).\n",
    "\n",
    "If you are working locally on your computer, please see the [README.md](https://github.com/nuitrcs/AI_Week_Topic_Model/blob/main/README.md) file on our GitHub repo for a command to create a conda environment that has the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08837cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    print(\"You are working in Google Colab.  We will install necessary packages...\")\n",
    "    !pip install scikit-learn sentence-transformers umap-learn hdbscan bertopic pandas matplotlib datashader bokeh holoviews scikit-image colorcet keybert\n",
    "except:\n",
    "    print(\"You are not working in Google Colab.\")\n",
    "    print(\"Please be sure that the necessary packages are installed and available, ideally within a conda env (e.g., see here: https://github.com/nuitrcs/AI_Week_Topic_Model/blob/main/README.md).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3584535f",
   "metadata": {},
   "source": [
    "In this exercise, you will join one of four groups: \n",
    "- Group 1: Embedding\n",
    "- Group 2: Dimension Reduction\n",
    "- Group 3: Clustering\n",
    "- Group 4: Labeling\n",
    " \n",
    "Each group will focus on the parameters in their own section (e.g., Group 1 will focus on changing parameters for the Embedding step), but will also run the entire topic modeling pipeline from start to finish, leaving parameters for the other steps on their default values. Each time you change a parameter in your section, please run the entire pipeline to see the resulting topics that are identified and labeled. Save the results of each run (e.g. save the result dataframe to your computer as a csv, or take a screen grab) so that you can remember the differences and share those with us in your presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5c3e04",
   "metadata": {},
   "source": [
    "### Read and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee629e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "# this is a function from sklearn that fetches the 20 newsgroups text dataset\n",
    "# it is a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups\n",
    "# this returns a bunch object, which is very similar to a dictionary\n",
    "bunch = fetch_20newsgroups(\n",
    "    categories=[\"comp.graphics\", \"rec.autos\", \"rec.motorcycles\", \n",
    "                \"rec.sport.baseball\", \"rec.sport.hockey\", \n",
    "                \"sci.electronics\", \"sci.med\", \"sci.space\"], # only extract select topics\n",
    "    remove=(\"headers\",\"footers\",\"quotes\")) # don't extract unnecessary metadata\n",
    "\n",
    "# get the text data and labels (in case you want to compare back to the original labels after you run through BERTopic)\n",
    "docs = bunch[\"data\"]\n",
    "doc_labels = bunch[\"target\"]\n",
    "# create a data frame with the text and labels\n",
    "df = pd.DataFrame({\n",
    "    \"text\": docs,\n",
    "    \"labels\": doc_labels\n",
    "})\n",
    "\n",
    "# create a label with text info\n",
    "df[\"labels_text\"] = df[\"labels\"].astype(\"category\").cat.rename_categories({i:j for i,j in enumerate(bunch[\"target_names\"])})\n",
    "\n",
    "# also remove documents that are empty\n",
    "df[\"text_processed\"] = df[\"text\"].str.strip()\n",
    "df = df[df[\"text_processed\"] != \"\"]\n",
    "\n",
    "print()\n",
    "print(\"Data Frame: \")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cd6bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the processed texts into docs variable\n",
    "docs = df[\"text_processed\"].values.tolist()\n",
    "print(docs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ced5467",
   "metadata": {},
   "source": [
    "## BERTopic Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce56533b",
   "metadata": {},
   "source": [
    "### Default Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23471e9d",
   "metadata": {},
   "source": [
    "The cell below defines default parameters and packages for each step of topic modeling. You can refer back to this as a reference when you start modifying your specific step below.  Everyone should run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29144d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "import umap.plot\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "# define default models and parameters\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "dimension_reduction_model = UMAP(n_components=2, n_neighbors = 15, metric=\"cosine\", random_state=54382)\n",
    "clustering_model = HDBSCAN(min_cluster_size=15, min_samples=1, cluster_selection_epsilon=0.165)\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# we will also run these models here so that you can have everything ready to go below\n",
    "# these steps aren't strictly necessary if you want to run everything directly through BERTopic but will help with this exercise\n",
    "embeddings = embedding_model.encode(docs) # encode the texts into embeddings\n",
    "dimension_reduction_model.fit(embeddings) # fit the UMAP model on the embeddings\n",
    "clustering_model.fit(dimension_reduction_model.embedding_) # fit the clustering model on the UMAP output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17558b3e",
   "metadata": {},
   "source": [
    "Once you customize the step in the pipeline that corresponds to your group, you should run the full topic model through the `BERTopic()` function shown below. You can copy this cell to your section below and use it to check how changes to your parameters affect the resulting topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c96353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define topic model pipeline with BERTopic\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=dimension_reduction_model,\n",
    "    hdbscan_model=clustering_model,\n",
    "    representation_model=representation_model\n",
    ")\n",
    "\n",
    "# begin topic modeling using the processed documents\n",
    "topic_model.fit(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f22807",
   "metadata": {},
   "source": [
    "### Visualize and Observe the Output of BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc81df2",
   "metadata": {},
   "source": [
    "Before you dive in to your specific group's section, please familiarize yourself with these methods to visualize and explore your results.  BERTopic model comes with a set of methods that you can use to observe the output of your pipeline.\n",
    "For more options, you can explore different chapters in the BERTopic documentation.\n",
    "\n",
    "- [Visualize topics](https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_topics.html)\n",
    "- [Visualize documents](https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_documents.html)\n",
    "- [Additional info on best practices](https://maartengr.github.io/BERTopic/getting_started/best_practices/best_practices.html)\n",
    "\n",
    "We provide some examples below. Try running these for yourself and see what you get! You can use these methods to understand how your choices of model & parameters can affect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34235675",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df199347",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c438d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the visualization with the original embeddings\n",
    "topic_model.visualize_documents(docs, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db1c29e",
   "metadata": {},
   "source": [
    "## Group 1: Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344f9e0",
   "metadata": {},
   "source": [
    "[Sentence transformer (also called SBERT)](https://sbert.net/index.html) is a widely used python package for accessing embedding models. You can find a selection of available models that can be used with SBERT [on this page of SBERT documentation](https://sbert.net/docs/sentence_transformer/pretrained_models.html#original-models). These models have been evaluated for their ability to produce high quality sentence embeddings. Choose a pretrained model for yourself and see how it affects the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5352d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "# all-MiniLM-L6-v2 is name of pretrained model\n",
    "# after you try this model, you should try others from the documentaiton linked above\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\") \n",
    "embeddings = embedding_model.encode(docs) # encode the texts into embeddings\n",
    "\n",
    "print(\"Dimension of embeddings: \",embeddings.shape)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e3aed9",
   "metadata": {},
   "source": [
    "Once you select a pretrained embedding model, run the topic modeling pipeline using the embedding model of your choice by copying the `BERTopic()` command provided above into the cell below and running it.  And don't forget to explore and visualize your results (e.g., using the example code from above)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f60e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy code from above to run BERTopic using your embedding model and then explore and visualize the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a19f6",
   "metadata": {},
   "source": [
    "## Group 2: Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc5a2ba",
   "metadata": {},
   "source": [
    "The output of the embedding model is a high dimensional array.  Attempting to cluster on those data would likely be difficult because the data points are sparse. We can perform dimension reduction to reduce the embeddings to 2 dimensions, which should preserve the useful information, and make the clustering step easier.\n",
    "\n",
    "[UMAP](https://umap-learn.readthedocs.io/en/latest/basic_usage.html) is a popular method for dimensionality reduction \n",
    "often used in text analysis. \n",
    "It works by preserving both local and global structures of the data in a lower-dimensional space. \n",
    "Several parameters control UMAPâ€™s behavior and output:\n",
    "\n",
    "- `n_neighbors`: Controls the balance between **local** and **global** structure.\n",
    "    - Lower values focus more on local structure, emphasizing relationships between nearby points, but may miss the global context.\n",
    "    - Higher values capture broader structures and global patterns, but may smooth out local details.\n",
    "- `n_components`: Specifies the target number of dimensions in the reduced space. This is typically set to 2 or 3.\n",
    "- `metric`: Defines the distance metric used to compute similarity between points. For text embeddings, `\"cosine\"` distance is commonly used.\n",
    "- `random_state`: Sets the seed for reproducibility. \n",
    "    Since UMAP involves stochastic processes (e.g., initialization and optimization), fixing the random_state ensures consistent results across runs.\n",
    "\n",
    "The cell below is a code for running UMAP on the embeddings calculated from the previous step. (Recall that we already define the `embedding_model` above.) Try out different parameters and plot the result until you find a reasonable configuration that groups the embeddings in a reasonable manner.\n",
    "\n",
    "For more detailed explanation of the parameters with examples, consult [this page](https://umap-learn.readthedocs.io/en/latest/parameters.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ecfc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize UMAP model\n",
    "# try changing the parameters one at a time\n",
    "dimension_reduction_model = UMAP(\n",
    "    n_components=2, \n",
    "    n_neighbors=15, \n",
    "    metric=\"cosine\", \n",
    "    random_state=54382\n",
    ")\n",
    "\n",
    "# fit the UMAP model to find the best 2D representation of the embeddings\n",
    "dimension_reduction_model.fit(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168fdc01",
   "metadata": {},
   "source": [
    "Once you fit the dimension reduction model, you can check the dimension reduction output with the following code. When you decide on the set of parameters, try running entire BERTopic model with your chosen dimension reduction model and see what you get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d658330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the UMAP representation\n",
    "umap.plot.points(dimension_reduction_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c1a3f",
   "metadata": {},
   "source": [
    "Once you select the parameters for your UMAP model, run the topic modeling pipeline using your UMAP model by copying the `BERTopic()` command provided above into the cell below and running it.  And don't forget to explore and visualize your results (e.g., using the example code from above)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d7b0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy code from above to run BERTopic using your embedding model and then explore and visualize the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43db22b",
   "metadata": {},
   "source": [
    "## Group 3:  Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac4519e",
   "metadata": {},
   "source": [
    "This group assumes that you have already selected embedding and dimension reduction methods and need to cluster our 2-D representation of the data. [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/index.html) is a popular method that used an unsupervised machine learning algorithm.  \n",
    "\n",
    "HDBSCAN provides many parameters that you can use to control the clustering behavior. You can read about the parameters in-depth [here](https://hdbscan.readthedocs.io/en/latest/parameter_selection.html). Important parameters in HDBSCAN that you will most commonly interact with are the following:\n",
    "\n",
    "- `min_cluster_size`: Specifies the smallest size grouping to be considered as a cluster. Increasing this value results in a smaller number of clusters.\n",
    "- `min_samples`: Provides a measure of how conservative you want your clustering to be. The larger the value, the more conservative the clustering, and therefore more points will be declared as noise, and clustering is restricted to more densely populated areas. \n",
    "- `cluster_selection_epsilon`: Helps with merging micro-clusters in regions where they are abundant, and ensures that clusters given below the threshold are not split further.\n",
    "\n",
    "Remember that we already defined the embeddings and reduced dimension output of those embeddings above.\n",
    "\n",
    "Your job is to modify the code below to cluster the way you think is the best. Try playing with different parameters. If you aren't able to get the output that you would like, try a different [clustering algorithm](https://scikit-learn.org/stable/modules/clustering.html). While HDBSCAN certain provides many advantages, you are not limited to only relying on that algorithm, and for a particular dataset other algorithm may perform better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91edc0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize HDBSCAN model\n",
    "# try changing the parameters one at a time\n",
    "clustering_model = HDBSCAN(\n",
    "    min_cluster_size=15, \n",
    "    min_samples=1, \n",
    "    cluster_selection_epsilon=0.165\n",
    ")\n",
    "\n",
    "# fit the clustering model to the reduced embeddings calculated in the previous step\n",
    "clustering_model.fit(dimension_reduction_model.embedding_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a50eab",
   "metadata": {},
   "source": [
    "Once you train the clustering model and label the datapoints, you can visually check the result using the `umap.plot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce319f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap.plot.points(dimension_reduction_model, labels=clustering_model.labels_, theme=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35669b7",
   "metadata": {},
   "source": [
    "Once you select the parameters for your HDBSCAN model, run the topic modeling pipeline using your HDBSCAN model by copying the `BERTopic()` command provided above into the cell below and running it.  And don't forget to explore and visualize your results (e.g., using the example code from above)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff48a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy code from above to run BERTopic using your embedding model and then explore and visualize the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb4a658",
   "metadata": {},
   "source": [
    "### Group 4: Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30ffd2",
   "metadata": {},
   "source": [
    "In this group, you will control how each cluster is labeled. The default BERTopic method uses a form of TF-IDF, but this doesn't always return optimal topic labels. We can improve by using a representation model implemented by BERTopic. The example below will demonstrate how to use a model inspired by KeyBERT, available in the BERTopic package, but you are free to try different methods as well.\n",
    "\n",
    "Recall that we already defined the default embedding, dimension reduction and clustering steps above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb354055",
   "metadata": {},
   "source": [
    "First, try running BERTopic without specifying a representation model. What kind of result do you get? Is this what we want? How can this be fixed? BERTopic explains how you can make better representations [here](https://maartengr.github.io/BERTopic/getting_started/best_practices/best_practices.html#improving-default-representation) and [here](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html). Try different approaches for yourself! You are not limited to what's provided in the BERTopic package, however. As we saw in the demo, you can utilize models outside of BERTopic to try to extract topics and keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c03c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define topic model pipeline with BERTopic\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=dimension_reduction_model,\n",
    "    hdbscan_model=clustering_model,\n",
    ")\n",
    "\n",
    "# begin topic modeling using the processed documents\n",
    "topic_model.fit(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126bbc2",
   "metadata": {},
   "source": [
    "Explore and visualize your results (e.g., using the example code from above)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6f80f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy code from above to explore and visualize the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ebeac9",
   "metadata": {},
   "source": [
    "Now let's compare with the `KeyBERTInspired` representation using the code below.  Afterwards, you can try a different representation model from the links provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde94ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a representation model, e.g., KeyBERTInspired()\n",
    "# after to run with KeyBERTInspired, you can choose something else \n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# define topic model pipeline with BERTopic\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=dimension_reduction_model,\n",
    "    hdbscan_model=clustering_model,\n",
    "    representation_model= representation_model\n",
    ")\n",
    "\n",
    "# begin topic modeling using the processed documents\n",
    "topic_model.fit(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8527bc7f",
   "metadata": {},
   "source": [
    "Don't forget to explore and visualize your results (e.g., using the example code from above)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy code from above to explore and visualize the results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
