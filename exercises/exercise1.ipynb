{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01aa8ac1",
   "metadata": {},
   "source": [
    "# BERTopic Exercise Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd14e36",
   "metadata": {},
   "source": [
    "If you are working on Colab, \n",
    "- The following cell installs all the packages you will need. \n",
    "- You may want to make use of the (free) GPU resources: click on the down arrow in the upper-right of the page next to the RAM and Disk usage graphic.  Then \"Change runtime type\" and select \"T4 GPU\".  This will dramatically speed up your runtime for this code.\n",
    "- Please be sure to save your file on your own account. (If you clicked on the link on our GitHub repo, your changes are not saved automatically).\n",
    "\n",
    "If you are working locally on your computer, please see the [README.md](https://github.com/nuitrcs/AI_Week_Topic_Model/blob/main/README.md) file on our GitHub repo for a command to create a conda environment that has the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08837cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    print(\"You are working in Google Colab.  We will install necessary packages...\")\n",
    "    !pip install scikit-learn sentence-transformers umap-learn hdbscan bertopic pandas matplotlib datashader bokeh holoviews scikit-image colorcet keybert\n",
    "except:\n",
    "    print(\"You are not working in Google Colab.\")\n",
    "    print(\"Please be sure that the necessary packages are installed and available, ideally within a conda env (e.g., see here: https://github.com/nuitrcs/AI_Week_Topic_Model/blob/main/README.md).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3584535f",
   "metadata": {},
   "source": [
    "![graphical representation of topic modeling pipeline](images/topic_modeling_pipeline.png)\n",
    "\n",
    "In this exercise, you will join one of four groups: \n",
    "- Group 1: Embedding\n",
    "- Group 2: Dimension reduction\n",
    "- Group 3: Clustering\n",
    "- Group 4: Labeling\n",
    " \n",
    "Each group will focus on the parameters in their own section (e.g., Group 1 will focus on changing parameters for the Embedding step), but will also run the entire topic modeling pipeline from start to finish, leaving parameters for the other steps on their default values. \n",
    "\n",
    "Each time you change a parameter in your section, please run the entire pipeline to see the resulting topics that are identified and labeled.  I have created a helpful function for you below to save you lines of code. \n",
    "\n",
    "Save the results of each run (e.g. save the result dataframe to your computer as a csv, or take a screen grab) so that you can remember the differences and share those with us in your presentation.  Try to gain some intuition about how the parameters affect the results of the topic modeling pipeline.\n",
    "\n",
    "Find the parameters that produce the best results for your step.  If you have extra time, feel free to explore other steps.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5c3e04",
   "metadata": {},
   "source": [
    "### (All Groups) Read in the Preprocessed Data\n",
    "\n",
    "For this demo, we will use the [`20newsgroups` dataset from scikit-learn](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset).  We will fetch the data and then clean reformat it so that it is easier for BERTopic to work with.  I wrote code to do this in the `supplementary_code.ipynb` notebook; that code also saves the output to a .csv file.  We can simply read in the resulting cleaned data below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee629e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# The line below will read the file directly from GitHub and will work on Colab.  \n",
    "df = pd.read_csv('https://raw.githubusercontent.com/nuitrcs/AI_Week_Topic_Modeling/refs/heads/main/exercises/data/sklearn_20newsgroups_cleaned.csv')\n",
    "# If you're working on your local computer and prefer to simply read the file from your disk, you can instead use the line below.\n",
    "# df = pd.read_csv('exercises/data/sklearn_20newsgroups_cleaned.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cd6bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the texts into docs variable as a list for use in BERTopic\n",
    "docs = df[\"cleaned_text\"].values.tolist()\n",
    "print(docs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce56533b",
   "metadata": {},
   "source": [
    "### (All Groups) Default Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23471e9d",
   "metadata": {},
   "source": [
    "The cell below defines default parameters and packages for each step of topic modeling. You can refer back to this as a reference when you start modifying your specific step below.  Everyone should run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29144d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "import umap.plot\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic.representation import KeyBERTInspired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d30bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define default models and parameters\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "dimension_reduction_model = UMAP(n_components=2, n_neighbors = 15, metric=\"cosine\", random_state=42)\n",
    "clustering_model = HDBSCAN(min_cluster_size=12, min_samples=5, cluster_selection_epsilon=0.2)\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# we will also run these models here so that you can have everything ready to go below\n",
    "# these steps aren't strictly necessary if you want to run everything directly through BERTopic but will help with this exercise\n",
    "embeddings = embedding_model.encode(docs) # encode the texts into embeddings\n",
    "dimension_reduction_model.fit(embeddings) # fit the UMAP model on the embeddings\n",
    "clustering_model.fit(dimension_reduction_model.embedding_) # fit the clustering model on the UMAP output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17558b3e",
   "metadata": {},
   "source": [
    "Once you customize the step in the pipeline that corresponds to your group, you should run the full topic model through the `BERTopic()` function shown below. I will create a wrapper function for you to reduce the number of lines you need to copy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c96353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runBERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    dimension_reduction_model=dimension_reduction_model,\n",
    "    clustering_model=clustering_model,\n",
    "    representation_model=representation_model\n",
    "):\n",
    "\n",
    "    # define topic model pipeline with BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=dimension_reduction_model,\n",
    "        hdbscan_model=clustering_model,\n",
    "        representation_model=representation_model\n",
    "    )\n",
    "\n",
    "    # begin topic modeling using the processed documents\n",
    "    topic_model.fit(docs)\n",
    "\n",
    "    return topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47409d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = runBERTopic() # using the defaults we defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f22807",
   "metadata": {},
   "source": [
    "### (All Groups) Visualize and Observe the Output of BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc81df2",
   "metadata": {},
   "source": [
    "Before you dive in to your specific group's section, please familiarize yourself with these methods to visualize and explore your results.  BERTopic model comes with a set of methods that you can use to observe the output of your pipeline.\n",
    "For more options, you can explore different chapters in the BERTopic documentation.\n",
    "\n",
    "- [Visualize topics](https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_topics.html)\n",
    "- [Visualize documents](https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_documents.html)\n",
    "- [Additional info on best practices](https://maartengr.github.io/BERTopic/getting_started/best_practices/best_practices.html)\n",
    "\n",
    "We provide some examples below. Try running these for yourself and see what you get! You can use these methods to understand how your choices of model & parameters can affect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34235675",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df199347",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c438d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the visualization with the original embeddings\n",
    "topic_model.visualize_documents(docs, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db1c29e",
   "metadata": {},
   "source": [
    "## Group 1: Embeddings\n",
    "\n",
    "![graphical representation of embedding step](images/embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344f9e0",
   "metadata": {},
   "source": [
    "[Sentence transformer (also called SBERT)](https://sbert.net/index.html) is a widely used python package for accessing embedding models. You can find a selection of available models that can be used with SBERT [on this page of SBERT documentation](https://sbert.net/docs/sentence_transformer/pretrained_models.html#original-models). These models have been evaluated for their ability to produce high quality sentence embeddings. Choose a pretrained model for yourself and see how it affects the output.  Also read the BERTopic documentation [here](https://maartengr.github.io/BERTopic/getting_started/embeddings/embeddings.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5352d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "# all-MiniLM-L6-v2 is name of the pretrained model we used above (you should change this model)\n",
    "# after you try this model, you should try others from the documentaiton linked above\n",
    "new_embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\") \n",
    "new_embeddings = new_embedding_model.encode(docs) # encode the texts into embeddings\n",
    "\n",
    "print(\"Dimension of embeddings: \",new_embeddings.shape)\n",
    "print(new_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e3aed9",
   "metadata": {},
   "source": [
    "Once you select a pretrained embedding model, run the topic modeling pipeline using the embedding model of your choice by copying the `BERTopic()` command provided above into the cell below and running it.  And don't forget to explore and visualize your results (e.g., using the example code from above)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f60e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model\n",
    "new_topic_model = runBERTopic(embedding_model=new_embedding_model) # using rest of the defaults we defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c12ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy code from above (or write your own) to explore and visualize the output\n",
    "# for example ...\n",
    "new_topic_model.visualize_documents(docs, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a19f6",
   "metadata": {},
   "source": [
    "## Group 2: Dimension Reduction\n",
    "\n",
    "![graphical representation of dimension reduction step](images/dimension_reduction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc5a2ba",
   "metadata": {},
   "source": [
    "The output of the embedding model is a high dimensional array.  Attempting to cluster on those data would likely be difficult because the data points are sparse. We can perform dimension reduction to reduce the embeddings to 2 dimensions, which should preserve the useful information, and make the clustering step easier.\n",
    "\n",
    "[UMAP](https://umap-learn.readthedocs.io/en/latest/basic_usage.html) is a popular method for dimensionality reduction \n",
    "often used in text analysis. \n",
    "It works by preserving both local and global structures of the data in a lower-dimensional space. \n",
    "Several parameters control UMAPâ€™s behavior and output:\n",
    "\n",
    "- `n_neighbors`: Controls the balance between **local** and **global** structure.\n",
    "    - Lower values focus more on local structure, emphasizing relationships between nearby points, but may miss the global context.\n",
    "    - Higher values capture broader structures and global patterns, but may smooth out local details.\n",
    "- `n_components`: Specifies the target number of dimensions in the reduced space. This is typically set to 2 or 3.\n",
    "- `metric`: Defines the distance metric used to compute similarity between points. For text embeddings, `\"cosine\"` distance is commonly used.\n",
    "- `random_state`: Sets the seed for reproducibility. \n",
    "    Since UMAP involves stochastic processes (e.g., initialization and optimization), fixing the random_state ensures consistent results across runs.\n",
    "\n",
    "The cell below is a code for running UMAP on the embeddings calculated from the previous step. (Recall that we already define the `embedding_model` above.) Try out different parameters and plot the result until you find a reasonable configuration that groups the embeddings in a reasonable manner.\n",
    "\n",
    "For more detailed explanation of the parameters with examples, consult [this page](https://umap-learn.readthedocs.io/en/latest/parameters.html).  Also read the BERTopic documentation [here](https://maartengr.github.io/BERTopic/getting_started/dim_reduction/dim_reduction.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ecfc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize UMAP model\n",
    "# try changing the parameters one at a time\n",
    "new_dimension_reduction_model = UMAP(\n",
    "    n_components=2, \n",
    "    n_neighbors=15, \n",
    "    metric=\"cosine\", \n",
    "    random_state=54382\n",
    ")\n",
    "\n",
    "# fit the UMAP model to find the best 2D representation of the embeddings\n",
    "new_dimension_reduction_model.fit(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168fdc01",
   "metadata": {},
   "source": [
    "Once you fit the dimension reduction model, you can check the dimension reduction output with the following code. When you decide on the set of parameters, try running entire BERTopic model with your chosen dimension reduction model and see what you get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d658330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the UMAP representation\n",
    "umap.plot.points(new_dimension_reduction_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c1a3f",
   "metadata": {},
   "source": [
    "Once you select the parameters for your UMAP model, run the topic modeling pipeline using your UMAP model by copying the `BERTopic()` command provided above into the cell below and running it.  And don't forget to explore and visualize your results (e.g., using the example code from above)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d7b0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model\n",
    "new_topic_model = runBERTopic(dimension_reduction_model=new_dimension_reduction_model) # using rest of the defaults we defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b95bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy code from above (or write your own) to explore and visualize the output\n",
    "# for example ...\n",
    "new_topic_model.visualize_documents(docs, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43db22b",
   "metadata": {},
   "source": [
    "## Group 3:  Clustering\n",
    "\n",
    "![graphical representation of clustering step](images/clustering.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac4519e",
   "metadata": {},
   "source": [
    "This group assumes that you have already selected embedding and dimension reduction methods and need to cluster our 2-D representation of the data. [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/index.html) is a popular method that used an unsupervised machine learning algorithm.  \n",
    "\n",
    "HDBSCAN provides many parameters that you can use to control the clustering behavior. You can read about the parameters in-depth [here](https://hdbscan.readthedocs.io/en/latest/parameter_selection.html). Important parameters in HDBSCAN that you will most commonly interact with are the following:\n",
    "\n",
    "- `min_cluster_size`: Specifies the smallest size grouping to be considered as a cluster. Increasing this value results in a smaller number of clusters.\n",
    "- `min_samples`: Provides a measure of how conservative you want your clustering to be. The larger the value, the more conservative the clustering, and therefore more points will be declared as noise, and clustering is restricted to more densely populated areas. \n",
    "- `cluster_selection_epsilon`: Helps with merging micro-clusters in regions where they are abundant, and ensures that clusters given below the threshold are not split further.\n",
    "\n",
    "Remember that we already defined the embeddings and reduced dimension output of those embeddings above.\n",
    "\n",
    "Your job is to modify the code below to cluster the way you think is the best. Try playing with different parameters. If you aren't able to get the output that you would like, try a different [clustering algorithm](https://scikit-learn.org/stable/modules/clustering.html). While HDBSCAN certain provides many advantages, you are not limited to only relying on that algorithm, and for a particular dataset other algorithm may perform better. Also, read the BERTopic documentation [here](https://maartengr.github.io/BERTopic/getting_started/clustering/clustering.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91edc0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize HDBSCAN model\n",
    "# try changing the parameters one at a time\n",
    "new_clustering_model = HDBSCAN(\n",
    "    min_cluster_size=15, \n",
    "    min_samples=1, \n",
    "    cluster_selection_epsilon=0.165\n",
    ")\n",
    "\n",
    "# fit the clustering model to the reduced embeddings calculated in the previous step\n",
    "new_clustering_model.fit(dimension_reduction_model.embedding_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a50eab",
   "metadata": {},
   "source": [
    "Once you train the clustering model and label the datapoints, you can visually check the result using the `umap.plot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce319f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap.plot.points(dimension_reduction_model, labels=new_clustering_model.labels_, theme=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35669b7",
   "metadata": {},
   "source": [
    "Once you select the parameters for your HDBSCAN model, run the topic modeling pipeline using your HDBSCAN model by copying the `BERTopic()` command provided above into the cell below and running it.  And don't forget to explore and visualize your results (e.g., using the example code from above)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff48a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model\n",
    "new_topic_model = runBERTopic(clustering_model=new_clustering_model) # using rest of the defaults we defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d2fa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy code from above (or write your own) to explore and visualize the output\n",
    "# for example ...\n",
    "new_topic_model.visualize_documents(docs, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb4a658",
   "metadata": {},
   "source": [
    "### Group 4: Labeling\n",
    "\n",
    "![graphical representation of labeling step](images/labeling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30ffd2",
   "metadata": {},
   "source": [
    "In this group, you will control how each cluster is labeled. The default BERTopic method uses a form of TF-IDF, but this doesn't always return optimal topic labels. We can improve by using a representation model implemented by BERTopic. The example below will demonstrate how to use a model inspired by KeyBERT, available in the BERTopic package, but you are free to try different methods as well.\n",
    "\n",
    "Recall that we already defined the default embedding, dimension reduction and clustering steps above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb354055",
   "metadata": {},
   "source": [
    "First, try running BERTopic without specifying a representation model. What kind of result do you get? Is this what we want? How can this be fixed? BERTopic explains how you can make better representations [here](https://maartengr.github.io/BERTopic/getting_started/best_practices/best_practices.html#improving-default-representation) and [here](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html). Try different approaches for yourself! You are not limited to what's provided in the BERTopic package, however. As we saw in the demo, you can utilize models outside of BERTopic to try to extract topics and keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d89479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_topic_model = runBERTopic(representation_model=None) # using rest of the defaults we defined above\n",
    "new_topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4937f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy code from above (or write your own) to explore and visualize the output\n",
    "# for example ...\n",
    "new_topic_model.visualize_documents(docs, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ebeac9",
   "metadata": {},
   "source": [
    "Now let's compare with the `KeyBERTInspired` representation we found using all the defaults (above).  Afterwards, you can try a different representation model from the links provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbae945",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_representation_model = #fill this in with a model that is discussed in the links provided above\n",
    "\n",
    "new2_topic_model = runBERTopic(representation_model=new_representation_model) # using rest of the defaults we defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8527bc7f",
   "metadata": {},
   "source": [
    "Don't forget to explore and visualize your results (e.g., using the example code from above)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy code from above to explore and visualize the results\n",
    "# for example ...\n",
    "new2_topic_model.visualize_documents(docs, embeddings=embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
